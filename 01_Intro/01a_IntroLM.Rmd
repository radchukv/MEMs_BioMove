---
title: "Introduction"
author: "Radchuk Viktoriia"
date: "2022-12-19"
output:
  revealjs::revealjs_presentation:
    self_contained: false
    reveal_options:
      slideNumber: true
      previewLinks: true
---


```{css, echo = FALSE}
.reveal ul {
  font-size: 30px;
}
.reveal ol {
  font-size: 30px;
  margin-bottom: -10px;
}
.reveal p {
 font-size: 30px;
  margin-bottom: -10px;
}
.reveal  pre {
  font-size: 14px;
  max-height: 300px;
  overflow-y: auto;
  margin-bottom: -3px;
}
.reveal code.r{
  font-size: 14px;
}

pre[class] {
  max-height: 100px;
}

.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(plotrix)
library(tidyverse)
library(ggplot2)
```

# Orga / Admin
We will meet between 10:00 and 15:00 in two sessions:

- 10:00 - 11:50, - morning session    
- 13:10 - 15:00, - afternoon session.      


Both the morning and the afternoon sessions will consist of a lecture and a practical exercise (each ~45 minutes).    
The practical exercise will be either guided by me or to be performed by you and we check the solution together afterwards.    


# Contents of the course
>
Day 1:    
>
 - Recap of linear model, diagnostics and inference    
- Intro to linear mixed-effects model     
- Random intercept and random slope models     

> 
Day 2:   
>
- Model diagnostics for mixed-effects models    
- Model inference, hypothesis testing     
- More complex random structures: temporal autocorrelation, non-Gaussian family     
- a teaser on multivariate stats   

# Contents of the course
Day 3:    

- Working on your own data / the data supplied by me      
- Clarifying the remaining questions      


# Linear model


<span style= 'font-size: 50px; color:orange'>*Repetition is the mother of all skill*</span>


# Graphical representation

Linear model can be depicted by a line that describes a functional relationship between at least two variables

* a predictor / explanatory / independent variable 
* and a response / dependent variable

```{r linear model, out.width= "60%", out.height= "60%"}
set.seed(5)
npoints <- 20
x <- rnorm(npoints, mean = 0, sd= 2)
y <- 2 + x*3 + rnorm(npoints, 0, 1)
plot(y ~ x, pch = 19, xlab = 'Predictor / Explanatory/',
     ylab = 'Response / dependent', xlim = c(min(x), max(x)))
mtext('independent', side = 1, line = 4)
abline(lm(y ~ x), col = 'blue')
```



# Mathematical representation

$y = \alpha + \beta\times x + \epsilon$, $\epsilon \sim N(0, \sigma^2)$    
<span style="color:green">$\alpha$</span> is intercept, i.e. the value of the response variable when the predictor = 0,

<span style = 'color:orange'>$\beta$</span> is slope, i.e. by how much the response variable is increasing 
per each unit of the predictor    

```{r linear model2, out.width= "50%", out.height= "50%"}
plot(y ~ x, pch = 19, xlab = 'Predictor / Explanatory/',
     ylab = 'Response / dependent', xlim = c(min(x), max(x)))
mtext('independent', side = 1, line = 4)
abline(lm(y ~ x), col = 'blue', lwd = 2)
segments(x0 = 0, y0 = 0, x1 = 0, y1 = 5, col = 'grey', lwd = 2)
segments(x0 = min(x), y0 = coef(lm(y ~ x))[1], x1 = max(x), y1 = coef(lm(y ~ x))[1], col = 'green', lwd = 2)

segments(x0 = 0.3, y0 = coef(lm(y ~ x))[1], x1 = 0.25, y1 = coef(lm(y ~ x))[1] + 0.6, col = 'orange', lwd = 3)


```

# Predicting a value
$y_i = \alpha + \beta\times x_i + \epsilon_i$;

$\epsilon_i \sim N(0, \sigma^2)$

```{r residuals, out.width= "60%", out.height= "60%"}
plot(y ~ x, pch = 19, xlab = 'Predictor / Explanatory/',
     ylab = 'Response / dependent')
mtext('independent', side = 1, line = 4)
abline(lm(y ~ x), col = 'blue')
points(x = c(2, 2), y = c(coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2], coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2), col = 'red', pch = 19)
segments(x0 = 2, y0 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2], x1 = 2, y1 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2, col = 'red')
text(x = 2.3, y = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 1, label = expression(paste(epsilon[i])), cex = 2, col = 'red')
arrows(x0 = -4, y0 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2, x1 = 2, y1 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2)
arrows(x0 = 2, y0 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2, x1 = -4, y1 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2)
text(x = -1, y = 11, label = expression(x[i]), cex = 2)

arrows(x0 = -2, y0 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2, x1 = -2, y1 = -10)
arrows(x0 = -2, y0 = -10, x1 = -2, y1 = coef(lm(y ~ x))[1] + 2*coef(lm(y ~ x))[2] + 2)
text(x = -2.5, y = 0, label = expression(y[i]), cex = 2)

```

# Assumtions of a linear model

- Normal distribution of residuals     
**in fact: normality of residuals at each X value**
- Additive error model
That means that errors cannot be related to the predictor variable in any other way than additive. In other words, the below models violate this assumption:
$$y = \alpha + \beta\times x \times  \epsilon$$
$$y = \alpha + \beta\times x^\epsilon$$

# Assumtions of a linear model

- iid: residuals are independent, identically distributed.
No correlation between residuals, e.g. due to autocorrelation
$Cov(\epsilon_i, \epsilon_j) = 0$
- Homoscedasticity: homogeneity of the variance of residuals  
```{r heterosced example, fig.align='center', out.height='20%'}
group1 <- rnorm(20, 2, sd = 1)
group2 <- rnorm(20, 2, sd = 3)
group3 <- rnorm(20, 2, sd = 4)

dat <- data.frame(Location = c(rep('Loc1', 20), rep('Loc2', 20), rep('Loc3', 20)), Weight = c(group1, group2, group3))
ggplot(dat, aes(x = Location, y = Weight)) +
  geom_boxplot() +
  geom_point(col = 'orange') + theme_bw()
```

# Before we go to an exercise and fit a linear model in R     
<span style = 'color:orange; font-size:50px;'>*Repetition is the mother of all skill*</span>    

#  Data types in R
R deals with objects.   

R has 6 basic data types:

- character: "F", "M";
- numeric (real or decimal): 1, 13.4;
- integer: 9L;  
- logical: FALSE, TRUE;
- complex: 2+3i;
- raw.

<span style="color:orange"> How can you check in R what is the type of the variable? </span> 



#  Data structures in R    

- atomic vector
- list

A vector is an ordered collection of basic data types.     
Atomic vectors must have all elements of the same type. E.g. there can be numeric, logical or character vectors.    
Lists are also vectors but they can have elements of different data types.    
```{r vectors, echo = TRUE}
(int_vec <- c(105L, 1L, 3L))
(log_vec <- c(TRUE, TRUE, FALSE))
(chr_vec <- c("a", "b", "d"))


```

#  Vectors & lists    
```{r lists, echo = TRUE}
class(int_vec); class(log_vec); class(chr_vec)
(lt <- list("d", 5:9, c(TRUE, TRUE, TRUE), c(0, 10.8)))

```


#  Data structures in R    

- atomic vector
- list
- matrix
- data frame
- factors

<span style="color:orange"> What is the difference between a matrix and a data frame? </span>  
<span style="color:orange"> What is the difference between character and factor?    
Why we often have to re-code character into a factor? </span>  

#  Data structures in R    
- A matrix is a rectangular arrangement of numbers in rows and columns. Matrices are two-dimensional structures, that contain data of the same type. In a matrix, rows run horizontally and columns run vertically.         
- A data frame is used for storing tabular data, so it is a two-dimensional structure. In fact, a data frame is a list of equal-length vectors. So, each column in a data frame is a vector. And these vectors can be of different data types (e.g. character, numeric, logical etc.).     

# Matrix & data frame
```{r df and mat, echo =  TRUE}
mat1 <- matrix(1:6, ncol = 2, nrow = 3)
mat1
dim(mat1)
df <- data.frame(x = 5:9, y = c("a", "x", "d", "r", "b"))
str(df)
```

#  Data structures in R    
A factor is a vector that stores categorical data with only predefined values.      
If you use a factor instead of a character vector you will quickly see if some levels have no observations 

```{r fact, echo = TRUE}
(f <- factor(c("F", "M", "M"))); class(f)
(sex_char <- c("F", "F", "F"))
sex_f <- factor(sex_char, levels = c("F", "M"))
```

# tidyverse changed the way we are coding in R

- tibble vs data frame
- %>% pipes the output of one function to the next one.    

<span style="color:orange"> What is the difference between a tibble and a data frame? </span>  

# Tibbles
Tibbles are 'improved' versions of data frames. 
Two main differences with data frames are how tiblles are printed and subset.   
If printed a tibble displays the data for the first 10 rows and the data type for each column.   
Difference in subsetting: `[` always returns another tibble, `[[` always returns a vector.  
```{r tibble, echo = TRUE, class.output="scroll-100"}
as_tibble(iris)
```


# Questions?  


# References
<span style = "font-size: 90%;">
<ul style='text-align: left;'> 
- Faraway JJ (2005) Linear Model with R. Chapman and Hall, New York, 255 p.   
- Dalgaard P (2008) Introductory Statistics with R. 2nd ed., Springer, 370 p.   
- Wickham H (2017) Advanced R. Chapman and Hall, 2nd ed. </span></ul>





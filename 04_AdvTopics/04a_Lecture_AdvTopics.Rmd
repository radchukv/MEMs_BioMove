---
title: "Mixed-effects models: advanced topics"
author: "Radchuk Viktoriia"
date: "2022-1-5"
output:
  revealjs::revealjs_presentation:
    self_contained: false
    incremental: true
    reveal_options:
      slideNumber: true
      previewLinks: true
       
---


```{css, echo = FALSE}
.reveal ul {
  font-size: 30px;
}
.reveal ol {
  font-size: 30px;
  margin-bottom: -10px;
}
.reveal p {
 font-size: 30px;
  margin-bottom: -10px;
}
.reveal  pre {
  font-size: 14px;
  max-height: 300px;
  overflow-y: auto;
  margin-bottom: -2px;
}
.reveal code.r{
  font-size: 14px;
}

pre[class] {
  max-height: 200px;
}

.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tidyverse)
library(lme4)
library(magrittr)
library(spaMM)
library(DHARMa)
library(glmmTMB)
```

# More complex random structures

Ecological data are often characterized by autocorrelation, either temporal or spatial (or both).   
Another commonly encountered structure is that levels of some factor are nested within those of the other, i.e. hierarchical model structures. For example, we sample species richness in 5 randomly selected plots at 7 different kettle holes. The plots are nested within the kettle holes. 

# Temporal autocorrelation
- Manifests itself in residuals.    
- Can be detected with Autocorrelation Function (ACF) or with formal tests (e.g. Durbin-Watson test).    
- If present, have to be accounted for, for example by including the autocorrelation structure of first order (AR1) in the residual variance.    

# Let us do it!
We use the data on relation between temperature and phenology, as we have already detected some issues with temporal autocorrelation there. 

```{r read&prep data, echo = TRUE}
dat <- read.csv(file = here::here('data', 'dat_phen_temp_subs.csv'))
dat_fac <- dat %>%
  mutate_if(is.character, as.factor) %>% 
  mutate(ID = as.factor(ID))
str(dat_fac)

```

# Fit the model with random slope    
```{r mod-spSlope spaMM, echo= TRUE}
mod <- fitme(Trait_mean ~ det_Clim + (1 + det_Clim|Species), data = dat_fac, method = 'REML')
summary(mod)
```

# Model diagnositcs with DHARMa
```{r simresid DHA, echo = TRUE, out.width='70%', fig.align='center'}
sim <- simulateResiduals(mod, plot = T)  

```

# Test of autocorrelation
We test autocorrelation per study. Let's look at study 21.    
```{r testAutocor, echo = TRUE, fig.align='center', out.width= '50%'}
sim_1 <- recalculateResiduals(sim, sel = dat_fac$ID == 21)
testTemporalAutocorrelation(sim_1, time = dat_fac$Year[dat_fac$ID == 21])

```

# Fit the model with AR1 structure
```{r mod-spSlope AR1 spaMM, echo= TRUE}
spaMM_AR1 <- fitme(Trait_mean ~ det_Clim + (1 + det_Clim|Species) + AR1(1|Year), data = dat_fac, method = 'REML')
summary(spaMM_AR1)
```

<span style = "font-size: 80%;"> The output now also contains the estimate of autocorrelation $\phi$ in residuals, which is rather high, 0.76.   
We also see the estimate of the variation due to the Year effect.   </span>

# The model with AR1 structure
- Notice that residual variance $\phi$ declines compared to the model without AR1 structure.    
- Important: by using syntax `AR1(1|Year)` we are assuming that autocorrelation is the same in each study. Ideally we should fit autocorrelation per study (since studies performed on different species / in different locations). To do it we use the syntax  `AR1(ID|Year)`.     
- <span style = 'color:orange'>Trying fitting it. What do you get?</span>


# Syntax with glmmTMB
```{r AR1 with glmmTMB, echo = TRUE}
dat_fac$Year_fac <- as.factor(dat_fac$Year)
TMB_AR1 <- glmmTMB(Trait_mean ~ det_Clim + (1 + det_Clim |Species) +  ar1(Year_fac - 1 | ID), data=dat_fac, 
                   control=glmmTMBControl(optimizer=optim,
                   optArgs=list(method="BFGS")))

```
Facing the reality   
<p style="text-align:center;">
<img src="img/CodeDoesntWork_Meme.jpg" height = "300"/>
</p>
<p style = "font-size: 40%;">http://www.quickmeme.com/meme/3uaxof</p>
 
# AR1 structure with glmmTMB
```{r out glmmTMB, echo  = TRUE}
summary(TMB_AR1)
```
<span style = "color:orange">Do you recall what were the signs of singular fit in mixed-effects models?</span>


# Model convergence problem
For more details see here: [Troubleshoot glmmTMB](https://cran.r-project.org/web/packages/glmmTMB/vignettes/troubleshooting.html)   
In this particular case the problem is too few levels of grouping variable (7 species)  


# Model diagnostics: spaMM
```{r simresid AR1 spaMM, echo = TRUE, fig.align='center', out.width='45%'}
sim_spam <- simulateResiduals(spaMM_AR1, plot = T)  

```
<span style= 'font-size:70%;'> Important: if you would test for residual autocorrelation, the result  will still be the same as before. You need to simulate the residuals in a different way (conditional on the fitted correlated random effects to properly assess autocorrelation in residuals after including autocorrelation structure in your model)</span>


# AIC and AICc

- $AIC = -2\times ln(L) + 2\times k$   
where _L_ is the maximum likelihood estimate of the model,    
and _k_ is the number of fitted parameters.      
- AIC was formulated by Hirotugu Akaike, a Japanese statistician.   
- A correction for small sample sizes: AICc
$AIC{c} = AIC + \dfrac{2 \times k \times (k +1)}{n - k - 1}$   
where n is the sample size.    
- 'Small sizes' are often defined as rule of thumb when $\dfrac{n}k < 40$.      
- Since AICc approximates AIC at large sample sizes, it is recommended to always use AICc.</span>   

# Model selection: why?
- There is a difference in philosophy behind __model selection__ based on AIC and classical __hypothesis testing__.   
- __Model selection__ aims to detect the 'best model' and is often used as means of generating hypotheses (rather exploratory analyses) or for making predictions.     
- __Hypothesis testing__ is used for inference and testing specific a-priori formulated hypotheses.    
- That is not to say that there are no hypotheses formulated when doing model selection, but these are not about the direction of impact of one predictor on the response variable but rather about the best group of predictors explaining the response variable.   

# Model selection: when?
Generally it is often used in observational studies, where no experiments can be set up to test a specific hypothesis. Such observational studies often have many covariates affecting the response variable.    
A specific field where model selection is often applied in habitat suitability analyses.

# Model selection: how?
- AIC can be used to compare diverse models (no constraints on models being nested within each other, as with LRT), as long as they are fitted to __EXACTLY__ the same data set.    
- AIC(c) of models are compared. The model that is supported by the data best is the one with the minimum AIC(c) given it is far away from other models in terms of their AIC(c). This distance to the 'next-best' model is defined as $\Delta AIC$.  

# Model selection: how?
- $\Delta AIC$ should be around 7-8 to consider that models differ in their support of the data. Though a $\Delta AIC$  threshold of 2 is often used in ecological literature, its origin is unclear, see e.g. Burnham et al. (2011).    
- If $\Delta AIC$ between models is < 7 then parsimony rule is typically applied to select the 'best' model, i.e. chosing the model among the candidate set that has the least parameters.  

# Model selection example
A hypothetical example   

| Model | AIC | $\Delta AIC$ | 
|---------------------|--------------------|---------------------|
| Temp + Temp2 + Precip + Precip2 + Year   | 230 | 0 | 
| Temp + Temp2 + Precip + Year   | 208 | 22 | 
| Temp + Temp2 + Precip + LandUse   | 206 | 24 |
| Temp  + Precip + LandUse   | 205.3 | 24.7 |


# Model selection: potential misuse
Some packages like MuMin make it all automatic but have to be applied thoughtfully.    
From Burnham & Anderson (2002): <span style = 'color:green'>"Careful, a priori consideration of alternative models will often require a major change in emphasis among many people.[...] This a priori strategy is in contrast to strategies advocated by others who view modeling and data analysis as a highly iterative and interactive exercise. Such a strategy, to us, represents deliberate data dredging and should be reserved for early exploratory phases of initial investigation. Such an exploratory avenue is not the subject of this book."</span>


# Multimodel inference: Model averaging, weighting etc
- Often there is no single model that is strongly supported by the data and is much better than the rest of the candidate models.    
- If data support for several models is very similar, model averaging is one approach to obtain the estimates for the parameters across those models.    
- BUT: this is a topic on its own.   


# Beyond normality: Generalised Linear Mixed-Effects Models
- We so far talked about Linear Mixed-Effects models, in which the process follows normal distribution.   
- There are many cases where variables are not following normal distribution.       
```{r non-norm, echo= FALSE, fig.align='center', out.width='55%'}
par(mfrow = c(1,2))
hist(rbinom(n = 100, size = 1, prob = 0.4), col ='black',
     xlab = 'Reproducing, yes/no', main = NULL)
hist(rpois(100, lambda= 3), col = 'black', 
xlab = 'Number of eggs laid', main = NULL)
```

# Generalised Linear Mixed-Effects Models
Three components of the generalised linear model:   

- the distribution of the response variable,   
- specification of the systematic component in terms of explanatory variables,   
- the link function, used to described the relation between the mean of the response variable and the systematic part.      

# Distributions
- Normal distribution   
- Poisson distribution    
- <span style = 'color:grey'> Negative Binomial distribution</span>            
- Binomial distribution    
- <span style = 'color:grey'> Gamma distribution</span>    
- <span style = 'color:grey'> Zero-inflated Poisson distribution</span>     
- <span style = 'color:grey'> Zero-inflated negative binomial distribution</span>   
- ...   

# Which distribution to use?
| Distribution | Data type  | Example    |
|---------------------|-------------------|----------------------|
| Normal              | Continuous; $(-\infty, \infty)$ |   Height, temperature, weight |
| Poisson             | Counts; $\geq 0, integer$      |  Number of offspring     |
| Negative Binomial   | Overdispersed counts; $\geq 0, integer$| Number of offspring, many 0s |
| Binomial            | Proportional data, presence/absence   | alive/dead, female/male        |
| Gamma               | Continuous;  $[0, \infty)$    |  Height, length                       |
| Zero-inflated Poisson | Overdispersed counts; $\geq 0, integer$ | Number of offspring, many 0s|


# Specification of the systematic component
Predictors affect a response variable via a linear predictor:
$$\eta =beta_0 + \beta_1 \times x_1 + \beta_2 \times x_2 + ... + \beta_p \times x_p $$

# Link function
Link function (g) specifies the function relating the linear predictor $\eta$ and the expected value of $Y$, that is mean response $\mu$    
$$\eta = g(\mu)$$      


# Poisson
link is log     
<p style='text-align: left;'>
1. $Y$ is Poisson-distributed with mean $\mu$     
$Y \sim P(\mu)$; $E(Y) = \mu$ and $var(Y) = \mu$        
2. Systematic part:     
$$\eta = beta_0 + \beta_1 \times x_1 +  ... + \beta_p \times x_p$$    
3. Logarithmic link is used between the mena of Y and the predictor $\eta$:    
$log(\mu) = \eta$    
Then $\mu = \exp(\eta)$, because exp() is an inverse of log().</p>>   



# Binomial
link is logit (though others are also possible, e.g. probit, log-log)   
<p style='text-align: left;'>
1. $Y$ is Binomial-distributed with probability $\pi$ and $n_i$ = 1 independent trial.         
2. Systematic part:    
$\eta =beta_0 + \beta_1 \times x_1 +  ... + \beta_p \times x_p$   
3. logit link is used between the mean of Y and the predictor $\eta$:    
$logit(\pi) = \eta$    
With some mathematics it follows that $\pi = \frac{exp(beta_0 + \beta_1 \times x_1 +  ... + \beta_p \times x_p)}{1+exp(beta_0 + \beta_1 \times x_1 +  ... + \beta_p \times x_p)}$.</p>> 


# Let's fit a GLMM in R!
```{r simu- Poisson RIdata}
SimulateMix <- function(intercept, slope, n, group.nb, var.group, var.error, times.perGroup){
  data <- data.frame(intercept = intercept, slope = slope, x = rnorm(n, 0, 2))
  data$group <- factor(rep(paste("group", 1:group.nb, sep = "_"), times.perGroup))
  data$b <- rep(rnorm(group.nb, mean = 0, sd = sqrt(var.group)), times.perGroup)
  data$error <- rnorm(n, mean = 0, sd = sqrt(var.error))
  
  data$lambda <- exp(intercept + data$slope*data$x + data$b + data$error)
  data$y <- unlist(lapply(1:length(data$lambda), function(x){rpois(n=1, lambda=data$lambda[x])})) # drawing the y from Poisson using lambda (linear predictor)
  data <- data[order(data$group),]
  return(data)
}

```

We simulate the data for offspring produced by different species of mammals. We have sampled offspring in 10 different species, recording the number of offspring for 20 individuals per species. Number of offspring is simulated to be negatively affected by temperature (x).    
```{r simu-data Pois, echo = TRUE}
set.seed(6)
dat_off <- SimulateMix(intercept = 0, slope = -0.6, n = 200, group.nb = 10, 
            var.group = 0.1, var.error = 0.05, times.perGroup = 20)
head(dat_off)
```

# Histogram of offspring number
```{r hist Offspr, echo = TRUE, fig.align='center', out.height= '50%', out.width='50%'}
hist(dat_off$y, xlab = 'Number of offspring', col = 'black', main = NULL) 
abline(v = mean(dat_off$y), col = 'red', lwd = 2)
```


# Poisson GLMM
We use function `glmer()` from the library `lme4`.   
```{r Pois fit, echo = TRUE}
modPois <- glmer(y ~ x + (1|group), data = dat_off, family = poisson(link = 'log'))
summary(modPois)
```
<span style = 'color:orange'>How do you interpret these results?</span>

# Model diagnostics
```{r modDiagn, echo =TRUE, fig.align='center', out.height= '50%', out.width='50%'}
sim <- simulateResiduals(modPois, plot = T)  
```
<span style = 'color:orange'>How do you interpret such results of model diagnostics?</span>

# But remember: reality can be ugly
<p style="text-align:center;">
<img src="img/CodeDoesntWork_Meme.jpg" height = "400"/>
</p>
<p style = "font-size: 40%;">http://www.quickmeme.com/meme/3uaxof</p>

# Overdispersion
- occurs if variation in the data is greater than would be expected under a given model.   
- often arises because some distributions have too few parameters to accommodate for the variability in observations. For example, in Poisson the variance = mean, constraining the variance in the model, which may not be the case in the data.    
- Overdispersion = Resid. Deviance / df.   
- Normally this ratio should be ~ 1, if it is >> 1 we talk about overdispersion.   

# Formal tests of overdispersion
```{r test Overdisp, echo = TRUE, fig.align='center', out.height= '50%', out.width='50%'}
testDispersion(sim)
```



# ML and Optimization
In order to find the parameter values that maximize the model likelihood optimizers are used. There are several of them. And the ones that are used by default may differ between the packages.
Main optimizers used in R:   

- Nelder-Mead:  a direct search algorithm, can be very slow, but often is very reliable.   
- BFGS: a Quasi-Newton (Hessian-based) algorithm.   
- L-BFGS-B: Limited memory-BFGS-Box constraint.   
- nlminb: a Quasi-Newton (Hessian-based) algorithm that supports box constraints.   
- bobyqa: Bound Optimization by Quadratic Approximation, a quadratic approximation algorithm that supports box constraints.  

# More on optimizers
- No single best optimizer exists.  
Possible reasons why an optimizer fails:   
- iterations can reach a singular gradient and will stop    
- iterations can exceed the maximum number of iterations without converging (can be dealt with my increasing the number of iterations)
- at least one of the variances is negative   
- covariance matrix is not positive definite.     
For more information see `lmerControl()`.


# Checking up
<span style = 'color:orange'> 
- What if you have underdispersion?   
- How to deal with overdispersion?   
</span>

# Questions?


# Literature
<span style = "font-size: 50%;">
<ul style='text-align: left;'> 
- Zuur AF, Ieno EN, Walker N, Saveliev AA, Smith GM 2009. Mixed-Effects Models and Extensions in Ecology with R. New York, Springer New York: XXII, 574 p.   
- Bolker BM, Brooks ME, Clark CJ, Geange SW, Poulsen JR, Stevens MHH,  White J-SS (2008) Generalized linear mixed models: a practical guide for ecology and evolution. _Trends in Ecology and Evolution_, 24:3.    
- Burnham KP, Anderson DR, Huyvaert KP (2011) AIC model selection and multimodel inference in behavioral ecology: some background, observations, and comparisons. _Behavioural Ecology and Sociobiology_, 65:23–35.    
- Burnham KP & Anderson DR (2002) Model Selection and Multimodel Inference: a practical information-theoretic approach. 2nd Ed, New York, Springer-Verlag: 515 p.    
- Anderson DR (2008) Model based inference in the life sciences: a primer on evidence. Springer, New York: 184 p.   
- Faraway JJ (2006) Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models  Chapman and Hall, New York, 345 p.   
- Tredennik AT, Hooker G, Ellner SP, Adler PB (2021) A practical guide to selecting models for exploration, inference, and prediction in ecology. _Ecology_, 0:e03336.</span></ul>









